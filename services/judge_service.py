"""
Judge service for getting feedback on LLM responses
"""
import time
from logging_config import get_logger

from .types import PromptMessage, AIResponse, Provider
from services import AIServiceFactory   
from database.service_factory import get_database_service

logger = get_logger("services.judge")

JUDGE_USER_PROMPT = """
Original User Prompt:
---
{user_prompt}
---
AI's Response to evaluate:
---
{original_response}
---
"""

class JudgeService:
    """Service for judging LLM responses"""
    
    def __init__(self, enable_database: bool = True):
        self.factory = AIServiceFactory()
        self.enable_database = enable_database
        self.db_service = get_database_service() if enable_database else None
        logger.info(f"JudgeService initialized with database enabled: {enable_database}")
        
    async def send_to_provider(self, provider: Provider, messages: list[PromptMessage], model: str = None, action: str = None):
        """Send prompt to a specific provider"""
        logger.debug(f"JudgeService sending to provider: {provider.value}")
        start_time = time.time()
        response = await self.factory.send_to_provider(provider, messages, model, action)
        response_time = time.time() - start_time
        
        logger.info(f"Judge response received from {provider.value} in {response_time:.2f}s")
               
        return response
        
    async def judge_response(self, provider: str, original_response: str, ui_request: str) -> AIResponse:
        """Judge an LLM response using another LLM"""
        logger.info(f"Judging response from provider: {provider}")
        
        user_prompt = JUDGE_USER_PROMPT.format(
            user_prompt=ui_request,
            original_response=original_response
        )
        system_prompt = """ You are an expert evaluator of AI-generated content.
        You will be given a user's prompt and the response generated by an AI model. 
        Your task is to critically evaluate the AI's response based on the user's prompt for a BrainWorkout.
        RATING SCALE ranges from 1 to 5. 1 being the worst and 5 being the best.
        """
        messages = [
            PromptMessage(role="system", content=system_prompt),
            PromptMessage(role="user", content=user_prompt)
        ]

        if provider.value == 'openai':
            logger.debug("Using Anthropic to judge OpenAI response")
            return await self.send_to_provider(Provider.ANTHROPIC, messages, action="judge_response")
        elif provider.value == 'anthropic':
            logger.debug("Using Gemini to judge Anthropic response")
            return await self.send_to_provider(Provider.GEMINI, messages, action="judge_response")
        elif provider.value == 'gemini':
            logger.debug("Using OpenAI to judge Gemini response")
            return await self.send_to_provider(Provider.OPENAI, messages, action="judge_response")
        else:
            logger.error(f"Unknown provider '{provider}' for judging")
            return AIResponse(
                provider="JudgeService",
                content="",
                model="",
                error=f"Unknown provider '{provider}' for judging."
            )
